# .env

# ── Model orchestration via Compose v2 “models” feature ──
# This is where you choose which model to pull & serve
LLM_MODEL_NAME=ai/qwen2.5

# ── Override the injected endpoint if you need to point elsewhere ──
# By default Compose will inject LLM_URL for you, but you can override:
LLM_URL=http://model-runner.docker.internal/engines/llama.cpp/v1

# ── OpenAI‑style API key (used in headers) ──
API_KEY=test

# ── Load test parameters ──
PROMPT="write me a 1000 word essay on AI"
NUM_CONCURRENT_USERS=1
MAX_TOKENS_PER_RESPONSE=512
